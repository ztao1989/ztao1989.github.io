<!doctype html>
<html>
<head>
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="chrome=1">
<title>ZTao's Homepage</title>
<link rel="stylesheet" href="stylesheets/styles-2.css">
</head>
<STYLE>A {text-decoration: none;} </STYLE>

<body>
<table border="0" width="61.8%" cellspacing="0" cellpadding="3" align="center"><tbody><tr><td>

<tr>
<td>
<table id="navigation" title="Navigation" border="0" cellspacing="0" cellpadding="6" align="left">
<tbody>
<tr>
<td><a href="index.html">Home</a></td>
<td><a href="research.html">Research</a></td>
<td><a href="publications.html">Publications</a></td>
<td><a href="teaching.html">Teaching</a></td>
<td><a href="group.html">Group</a></td>
<td><a href="services.html">Services</a></td>
</tr>
</tbody>
</table>
</td>
</tr>

<tr><td><hr />

<h2 style="margin:20px 0 10px"> Research </h2>

<p> 
Some of our recent research interests are listed as follows.</p>
<ul>
<li> Reliable Large Vision-Language Understanding</li>
<li> Uncertainty Quantification in Hybrid Models</li>
<li> Robust Sparse Network Training</li>
<li> Fair Ranking Systems Against Social Bias</li>
</ul>

<tr><td><hr />

<p></p>
<h3> Reliable Large Vision-Language Understanding </h3>
<p><img src="images/SQ-LLaVA.jpg" width="320" style="float:right; margin:0px 10px 0px 0px;" /></p>
<p>We study the reliability and robustness of large language/multimodal models (LLM/LMM) and vision-language (VL) embeddings when applying these fundamental models in new domains. We investigate how to incorporate uncertainties into VL models and tasks, such as general purpose VL understanding, multi-round conversation, video/image-text retrieval, and implement new training insights to improve data efficiency for fine-tuning LLMs through visual instruction tuning, such as self-questioning that enables LLMs automatic in-context learning and visual discovery. The visual self-questioning applies to various LLM/LMM architectures, including Llama 3, Qwen2-VL, etc., and consistently improves the VL performance on 0.5B, 7B/8B, and 13B model sizes. 
</p>

<p> 
	We also apply our research in biomedical and healthcare applications. One of our recent works designs a new self-training LLaVA model (STLLaVA-Med) that is capable of learning to ask relevant medical questions and leveraging direct preference optimization (DPO) to enhance expert knowledge. Based on self-training, the model gets rid of large medical data pre-training and only requires a small amount of biomedical preference data labeled by closed-source LLM API (e.g., GPT-4o). Empirically, our model can be used in medical visual question answering, report generation, general medicine assistance, etc. 
</p>
<p><img src="images/ST-LLaVA-Med.jpg" width="560" style="float:right; margin:10px 0px 0px 0px;" /></p>

<ul>	
	<li>
	Guohao Sun, Can Qin, Huazhu Fu, Linwei Wang, and <strong>Zhiqiang Tao</strong>, "Self-Training Large Language and Vision Assistant for Medical", EMNLP, 2024.
	</li>

	<li>
	Guohao Sun, Can Qin, Jiamian Wang, Zeyuan Chen, Ran Xu, and <strong>Zhiqiang Tao</strong>, "SQ-LLaVA: Self-Questioning for Large Vision-Language Assistant", ECCV, 2024.
	</li>

	<li>
	Jiamian Wang, Pichao Wang, Dongfang Liu, Qiang Guan, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and <strong>Zhiqiang Tao</strong>, "Diffusion-Inspired Truncated Sampler for Text-Video Retrieval", NeurIPS, 2024.
	</li>

	<li>
	Jiamian Wang, Guohao Sun, Pichao Wang, Dongfang Liu, Sohail Dianat, Majid Rabbani, Raghuveer Rao, and <strong>Zhiqiang Tao</strong>, "Text Is MASS: Modeling as Stochastic Embedding for Text-Video Retrieval", CVPR, 2024
	</li>

	<li>
	Guohao Sun, Yue Bai, Xueying Yang, Yi Fang, Yun Fu, and <strong>Zhiqiang Tao</strong>, "Aligning Out-of-Distribution Web Images and Caption Semantics via Evidential Learning", ACM Web Conference (WWW), 2024.  
	</li>
</ul>

<hr />
<p></p>

<h3>Uncertainty Quantification in Hybrid Models</h3>

<p><img src="images/CASSI.jpg" width="380" style="float:right; margin:0px 10px 0px 0px;" /></p>
<p>
To bridge the gap between lab simulation and real environments, we explore and model uncertainties for data-driven hybrid systems (e.g., physic-informed models, snapshot compressive imaging, etc.) at various levels by developing performance guarantees for co-optimized hardware and software and investigating scalable approximate inference techniques. We have built a new unified bilevel optimization framework through a Bayes lens to capture hardware, model, and data uncertainties in multiple complex systems, including hyperspectral imaging, video compression, and phase retrieval. One key insight of our proposed research is to parameterize hardware as hyperparameters – to realize co-optimization – and qualify its uncertainties through hyperparameter optimization techniques. 
</p>

<p><img src="images/Hybrid-UQ.jpg" width="300" style="float:right; margin:0px 10px 0px 0px;" /></p>

<ul>
<li>
Jiamian Wang, Zongliang Wu, Yulun Zhang, Xin Yuan, Tao Lin, and <strong>Zhiqiang Tao</strong>, "Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging", NeurIPS, 2024.
</li>

<li>
Jiamian Wang, Yulun Zhang, Xin Yuan, Ziyi Meng, and <strong>Zhiqiang Tao</strong>, "Modeling Mask Uncertainty in Hyperspectral Image Reconstruction", ECCV, 2022.
</li>

<li>
Xueying Yang, Jiamian Wang, Xujiang Zhao, Sheng Li, and <strong>Zhiqiang Tao</strong>, 
"Calibrate Automated Graph Neural Network via Hyperparameter Uncertainty", ACM CIKM, 2022.
</li>

<li>
<strong>Zhiqiang Tao</strong>, Yaliang Li, Bolin Ding, Ce Zhang, Jingren Zhou, and Yun Fu, "Learning to Mutate with Hypergradient Guided Population", NeurIPS, 2020.
</li>

</ul>

<hr />
<p></p>


<h3>Robust Sparse Network Training </h3>

<p><img src="images/sparse.jpg" width="480" style="float:right; margin:0px 10px 0px 0px;" /></p>
<p>
Deep neural networks, especially for low-level image restoration tasks, suffer from a high model complexity and cannot provide calibrated uncertainty estimates for safety-critical problems, such as medical imaging, remote sensing, navigation, etc. We aim to develop sparse network training algorithms to empower lightweight and trustworthy models. The key contribution of our research lies in improving the trainability of sparse subnetworks, especially in larger pruning ratios, and investigating the robustness of training sparse networks with random weight initialization to express and calibrate model uncertainty without sacrificing fidelity and inference efficiency.
</p>

<ul>
	<li>
	Jiamian Wang, Huan Wang, Yulun Zhang, Yun Fu, and <strong>Zhiqiang Tao</strong>, "Iterative Soft Shrinkage Learning for Efficient Image Super-Resolution", ICCV, 2023.
	</li>

	<li>
	Hitesh Sapkota, Dingrong Wang, <strong>Zhiqiang Tao</strong>, and Qi Yu, "Distributionally Robust Ensemble of Lottery Tickets Towards Calibrated Sparse Network Training", NeurIPS, 2023.
	</li>

	<li>
	Yue Bai, Huan Wang, Xu Ma, Yitian Zhang, <strong>Zhiqiang Tao</strong>, and Yun Fu, "Parameter-efficient masking networks", NeurIPS, 2022. 
	</li>

	<li>
	Yue Bai, Huan Wang, <strong>Zhiqiang Tao</strong>, Kunpeng Li, and Yun Fu, "Dual Lottery Ticket Hypothesis", ICLR, 2022. 
	</li>
</ul>


<!-- <p><img src="images/logseq.jpg" width="320" style="float:left; margin:0px 10px 0px 0px;" /></p>
<p> Our research study is one of the pioneering attempts to develop an interpretable user model upon the user log history along with auxiliary software tutorials. We build on the top of a sequence-to-sequence model, and designs two encoding pathways to jointly capture the temporal context from user log sequences and the semantic information from tutorial text annotations. We propose a recurrent memory network to link these two sources, which recurrently queries the tutorial memories with the temporal context, and thus interprets the user log with attention mechanism. A recurrent neural network based decoder is used to predict the user next action. Our study shows the great potential of using log-trace data for the user personalization tasks, such as detecting user interest across platforms.
</p>
 -->

<hr />
<p></p>

<h3>Fair Ranking Systems Against Social Bias</h3>

<p><img src="images/fair-ranking.jpg" width="230" style="float:right; margin:0px 10px 0px 0px;" /></p>
<p>
	Modern AI-powered search systems could make unfair decisions about demographic groups that infrequently appear in the training dataset. This data bias is usually induced by the skewed distribution of social-biased attributes (e.g., race, gender, and religion). Our research empirically studies various data bias issues and extensively designs mitigation methods to achieve fair ranking by meta-learning, curriculum learning, and prompt tuning.
</p>

<ul>
	<li>
	Yuan Wang, Xuyang Wu, Hsin-Tai Wu, <strong>Zhiqiang Tao</strong>, and Yi Fang, "Do Large Language Models Rank Fairly? An Empirical Study on the Fairness of LLMs as Rankers", NAACL, 2024.
	</li>

	<li>
	Yuan Wang, <strong>Zhiqiang Tao</strong>, and Yi Fang, "A Unified Meta-learning Framework for Fair Ranking with Curriculum Learning", IEEE Transactions on Knowledge and Data Engineering (TKDE), 2024.
	</li>

	<li>
	Yuan Wang, Peifeng Yin, <strong>Zhiqiang Tao</strong>, Hari Venkatesan, Jin Lai, Yi Fang, and PJ Xiao, "An Empirical Study of Selection Bias in Pinterest Ads Retrieval", KDD, 2023.
	</li>

	<li>
	Yuan Wang, <strong>Zhiqiang Tao</strong>, and Yi Fang, "A Meta-learning Approach to Fair Ranking", ACM SIGIR, 2022.
	</li>

	<li>
	Yi Fang, Hongfu Liu, <strong>Zhiqiang Tao</strong>, Mikhail Yurochkin, "Fairness of Machine Learning in Search Engines", ACM CIKM, 2022.
	</li>

</ul>


</tbody>
</table>
<h1>&nbsp;</h1>

<script src="javascripts/scale.fix.js"></script>
</body>
</html>